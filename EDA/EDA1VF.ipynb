{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for data analysis and visualization\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt  # For creating various plots and visualizations\n",
    "import numpy as np\n",
    "import pandas as pd  # For handling and manipulating structured data (tables, CSV, etc.)\n",
    "import seaborn as sns  # For advanced data visualization with statistical capabilities\n",
    "# Importing automatic data profiling tool\n",
    "import ydata_profiling as yd  # Used to generate a report with statistics, correlations, and distributions for data exploration\n",
    "# Importing time series analysis tools from pandas and statsmodels\n",
    "from pandas.plotting import lag_plot  # For visualizing lag correlations in time series data\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "# Importing seasonal decomposition tool for time series analysis\n",
    "from statsmodels.tsa.seasonal import  seasonal_decompose  # To decompose a time series into trend, seasonality, and residual components\n",
    "# Importing statistical test for stationarity\n",
    "from statsmodels.tsa.stattools import  adfuller  # Augmented Dickey-Fuller (ADF) test to check stationarity of a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 'Date' as the datetime index.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>number_sold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-29</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230090 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            store  product  number_sold\n",
       "Date                                   \n",
       "2010-01-01      0        0          801\n",
       "2010-01-02      0        0          810\n",
       "2010-01-03      0        0          818\n",
       "2010-01-04      0        0          796\n",
       "2010-01-05      0        0          808\n",
       "...           ...      ...          ...\n",
       "2018-12-27      6        9          890\n",
       "2018-12-28      6        9          892\n",
       "2018-12-29      6        9          895\n",
       "2018-12-30      6        9          899\n",
       "2018-12-31      6        9          912\n",
       "\n",
       "[230090 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"../datasets/Train.csv\",\n",
    "    parse_dates=True,\n",
    ")\n",
    "# Try to detect a datetime column\n",
    "for col in df.columns:\n",
    "    df[col] = pd.to_datetime(\n",
    "        df[col], errors=\"coerce\"\n",
    "    )  # Convert to datetime if possible\n",
    "    if is_datetime64_any_dtype(df[col]):\n",
    "        df.set_index(col, inplace=True)\n",
    "        print(f\"Set '{col}' as the datetime index.\")\n",
    "        break \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236fe39cb1964f72a37e155ba8001437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d43df0ea2c4255a40a654388fba0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render JSON:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84016b2444c4b83a0193b5780349697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d13c61cca7242db8a980cc87fa0fc4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16f7f393d4b446dad2571c793f89698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a09221872046f0b7a4f111215359ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DataProfile = yd.ProfileReport(df)\n",
    "DataProfile.to_file(\"Profile.json\")\n",
    "DataProfile.to_file(\"Profile.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            store  product  number_sold\n",
      "Date                                   \n",
      "2010-01-01      0        0          801\n",
      "2010-01-02      0        0          810\n",
      "2010-01-03      0        0          818\n",
      "2010-01-04      0        0          796\n",
      "2010-01-05      0        0          808\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 230090 entries, 2010-01-01 to 2018-12-31\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype\n",
      "---  ------       --------------   -----\n",
      " 0   store        230090 non-null  int64\n",
      " 1   product      230090 non-null  int64\n",
      " 2   number_sold  230090 non-null  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 7.0 MB\n",
      "None\n",
      "               store        product    number_sold\n",
      "count  230090.000000  230090.000000  230090.000000\n",
      "mean        3.000000       4.500000     780.926107\n",
      "std         2.000004       2.872288     204.096737\n",
      "min         0.000000       0.000000     238.000000\n",
      "25%         1.000000       2.000000     722.000000\n",
      "50%         3.000000       4.500000     835.000000\n",
      "75%         5.000000       7.000000     914.000000\n",
      "max         6.000000       9.000000    1205.000000\n"
     ]
    }
   ],
   "source": [
    "# Basic exploration\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "\n",
    "    for column in df.columns:\n",
    "        df[column] = pd.to_numeric(\n",
    "            df[column],\n",
    "            errors=\"coerce\",\n",
    "        )\n",
    "\n",
    "        if df[column].isna().sum() > 0 or np.isinf(df[column]).sum() > 0:\n",
    "\n",
    "            print(f\"Handling missing values in {column}\")\n",
    "\n",
    "\n",
    "            # Remplacer les valeurs infinies par NaN\n",
    "\n",
    "\n",
    "            df[column] = df[column].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "            # Calcul du ratio de valeurs manquantes\n",
    "\n",
    "            missing_ratio = df[column].isna().sum() / len(df)\n",
    "\n",
    "\n",
    "            # Choix de la stratégie selon le pourcentage de NaN\n",
    "\n",
    "\n",
    "            if missing_ratio < 0.05:\n",
    "\n",
    "                strategy = \"mean\"  # Moins de 5% → Remplacement par la moyenne\n",
    "\n",
    "                df[column].fillna(df[column].mean(), inplace=True)\n",
    "\n",
    "            elif missing_ratio < 0.2:\n",
    "\n",
    "\n",
    "                strategy = \"interpolation\"  # Entre 5% et 20% → Interpolation linéaire\n",
    "\n",
    "                df[column].interpolate(method=\"linear\", inplace=True)\n",
    "\n",
    "            else:\n",
    "\n",
    "\n",
    "                strategy = \"median\"  # Plus de 20% → Remplacement par la médiane\n",
    "\n",
    "                df[column].fillna(df[column].median(), inplace=True)\n",
    "\n",
    "\n",
    "            print(f\"Applied {strategy} strategy for {column}\")\n",
    "\n",
    "    df.to_csv(\"df.csv\")\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# def convert_if_majority_numeric(dataframe):\n",
    "#     # print(dataframe)\n",
    "#     df1 = dataframe\n",
    "#     df1.dropna(inplace=True)\n",
    "#     \"\"\"Convert string numbers to numeric values only if most elements are numeric.\"\"\"\n",
    "#     for column in df1.columns:\n",
    "#         type_counts = df[column].apply(lambda x: is_numeric_dtype(x)).value_counts()\n",
    "#         for x in df[column]:\n",
    "#             print(type(x))\n",
    "\n",
    "#         # Check if majority is numeric\n",
    "#         majority_is_numeric = type_counts.get(True, 0) > type_counts.get(False, 0)\n",
    "#         print(column, \" : \", type_counts.get(True, 0))\n",
    "#         print(\n",
    "#             column,\n",
    "#             \" : \",\n",
    "#         )\n",
    "#         print(column, \" : \", majority_is_numeric)\n",
    "\n",
    "#         if majority_is_numeric:\n",
    "#             df[column] = pd.to_numeric(\n",
    "#                 df[column],\n",
    "#                 errors=\"coerce\",\n",
    "#             )  # Convert only if majority is numeric\n",
    "#             print(\"converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m report\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Generate and save the report\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m final_report \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_mlops_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Save to JSON\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlops_eda_report.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[9], line 37\u001b[0m, in \u001b[0;36mgenerate_mlops_report\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Stationarity Analysis\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 37\u001b[0m     adf_result \u001b[38;5;241m=\u001b[39m \u001b[43madfuller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     report[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessing_recommendations\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstationarity\u001b[39m\u001b[38;5;124m\"\u001b[39m][column] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_stationary\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(adf_result[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.05\u001b[39m),\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_value\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(adf_result[\u001b[38;5;241m1\u001b[39m]),\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformation_needed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m adf_result[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m     }\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Recommend transformations if not stationary\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\statsmodels\\tsa\\stattools.py:326\u001b[0m, in \u001b[0;36madfuller\u001b[1;34m(x, maxlag, regression, autolag, store, regresults)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;66;03m# 1 for level\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# search for lag length with smallest information criteria\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# Note: use the same number of observations to have comparable IC\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# aic and bic: smaller is better\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m regresults:\n\u001b[1;32m--> 326\u001b[0m     icbest, bestlag \u001b[38;5;241m=\u001b[39m \u001b[43m_autolag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mOLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxdshort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullRHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstartlag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautolag\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     icbest, bestlag, alres \u001b[38;5;241m=\u001b[39m _autolag(\n\u001b[0;32m    331\u001b[0m         OLS,\n\u001b[0;32m    332\u001b[0m         xdshort,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m         regresults\u001b[38;5;241m=\u001b[39mregresults,\n\u001b[0;32m    338\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\statsmodels\\tsa\\stattools.py:133\u001b[0m, in \u001b[0;36m_autolag\u001b[1;34m(mod, endog, exog, startlag, maxlag, method, modargs, fitargs, regresults)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(startlag, startlag \u001b[38;5;241m+\u001b[39m maxlag \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    132\u001b[0m     mod_instance \u001b[38;5;241m=\u001b[39m mod(endog, exog[:, :lag], \u001b[38;5;241m*\u001b[39mmodargs)\n\u001b[1;32m--> 133\u001b[0m     results[lag] \u001b[38;5;241m=\u001b[39m \u001b[43mmod_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    136\u001b[0m     icbest, bestlag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m((v\u001b[38;5;241m.\u001b[39maic, k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\statsmodels\\regression\\linear_model.py:333\u001b[0m, in \u001b[0;36mRegressionModel.fit\u001b[1;34m(self, method, cov_type, cov_kwds, use_t, **kwargs)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpinv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    330\u001b[0m             \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalized_cov_params\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    331\u001b[0m             \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m--> 333\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog, singular_values \u001b[38;5;241m=\u001b[39m \u001b[43mpinv_extended\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwexog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized_cov_params \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\n\u001b[0;32m    335\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog, np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog))\n\u001b[0;32m    337\u001b[0m         \u001b[38;5;66;03m# Cache these singular values for use later.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\statsmodels\\tools\\tools.py:274\u001b[0m, in \u001b[0;36mpinv_extended\u001b[1;34m(x, rcond)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         s[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m--> 274\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res, s_orig\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = handle_missing_values(df)\n",
    "def generate_mlops_report(df):\n",
    "    \"\"\"\n",
    "    Generate a machine-interpretable EDA report for MLOps preprocessing\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input time series dataframe\n",
    "    \n",
    "    Returns:\n",
    "    dict: Structured, machine-readable analysis report\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        \"dataset_metadata\": {\n",
    "            \"total_columns\": len(df.columns),\n",
    "            \"total_rows\": len(df),\n",
    "            \"date_range\": {\n",
    "                \"start\": str(df.index.min()),\n",
    "                \"end\": str(df.index.max())\n",
    "            }\n",
    "        },\n",
    "        \"preprocessing_recommendations\": {\n",
    "            \"stationarity\": {},\n",
    "            \"feature_scaling\": [],\n",
    "            \"feature_engineering\": []\n",
    "        },\n",
    "        \"statistical_insights\": {\n",
    "            \"descriptive_stats\": {},\n",
    "            \"correlations\": {\n",
    "                \"significant_correlations\": [],\n",
    "                \"correlation_matrix\": {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Stationarity Analysis\n",
    "    for column in df.columns:\n",
    "        adf_result = adfuller(df[column])\n",
    "        report[\"preprocessing_recommendations\"][\"stationarity\"][column] = {\n",
    "            \"is_stationary\": str(adf_result[1] < 0.05),\n",
    "            \"p_value\": float(adf_result[1]),\n",
    "            \"transformation_needed\": \"YES\" if adf_result[1] >= 0.05 else \"NO\"\n",
    "        }\n",
    "        \n",
    "        # Recommend transformations if not stationary\n",
    "        if adf_result[1] >= 0.05:\n",
    "            report[\"preprocessing_recommendations\"][\"feature_engineering\"].append({\n",
    "                \"column\": column,\n",
    "                \"suggested_transformations\": [\n",
    "                    \"log_transformation\",\n",
    "                    \"differencing\",\n",
    "                    \"rolling_mean_normalization\"\n",
    "                ]\n",
    "            })\n",
    "    \n",
    "    # Descriptive Statistics\n",
    "    for column in df.columns:\n",
    "        report[\"statistical_insights\"][\"descriptive_stats\"][column] = {\n",
    "            \"mean\": float(df[column].mean()),\n",
    "            \"std\": float(df[column].std()),\n",
    "            \"min\": float(df[column].min()),\n",
    "            \"max\": float(df[column].max())\n",
    "        }\n",
    "    \n",
    "    # Correlation Analysis\n",
    "    corr_matrix = df.corr()\n",
    "    report[\"statistical_insights\"][\"correlations\"][\"correlation_matrix\"] = \\\n",
    "        {str(col): {str(subcol): float(corr_matrix.loc[col, subcol]) \n",
    "                    for subcol in df.columns} \n",
    "         for col in df.columns}\n",
    "    \n",
    "    # Significant Correlations\n",
    "    for col1 in df.columns:\n",
    "        for col2 in df.columns:\n",
    "            if col1 != col2:\n",
    "                correlation = float(corr_matrix.loc[col1, col2])\n",
    "                if abs(correlation) > 0.5:\n",
    "                    report[\"statistical_insights\"][\"correlations\"][\"significant_correlations\"].append({\n",
    "                        \"features\": [col1, col2],\n",
    "                        \"correlation\": correlation,\n",
    "                        \"strength\": \"strong\" if abs(correlation) > 0.7 else \"moderate\"\n",
    "                    })\n",
    "    \n",
    "    # Feature Scaling Recommendations\n",
    "    for column in df.columns:\n",
    "        if df[column].std() > 1:  # Suggest scaling for features with high variance\n",
    "            report[\"preprocessing_recommendations\"][\"feature_scaling\"].append({\n",
    "                \"column\": column,\n",
    "                \"recommended_method\": [\"standardization\", \"min_max_scaling\"]\n",
    "            })\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and save the report\n",
    "final_report = generate_mlops_report(df)\n",
    "\n",
    "# Save to JSON\n",
    "with open('mlops_eda_report.json', 'w') as f:\n",
    "    json.dump(final_report, f, indent=4)\n",
    "\n",
    "print(\"MLOps-friendly EDA Report generated and saved to 'mlops_eda_report.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    plt.figure(df.columns.get_loc(column)+1,figsize=(18, 18))\n",
    "    # Plot the time series data for the current column\n",
    "    plt.plot(df.index, df[column], label=column)\n",
    "plt.title('Time Series Plot')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop through each column in the DataFrame to perform stationarity testing and seasonal decomposition\n",
    "for column in df.columns:\n",
    "    print(f\"\\nAnalyzing column: {column}\")  # Print the name of the column being analyzed\n",
    "    \n",
    "    # Create a new figure for the decomposition plot\n",
    "    plt.figure(figsize=(20, 14))\n",
    "\n",
    "    # --- Augmented Dickey-Fuller (ADF) Test for Stationarity ---\n",
    "    adf_result = adfuller(df[column])  # Perform the ADF test on the current column\n",
    "    print(f\"ADF Statistic for {column}: {adf_result[0]}\")  # Print the test statistic\n",
    "    print(f\"p-value for {column}: {adf_result[1]}\")  # Print the p-value (to check stationarity)\n",
    "    \n",
    "    # The ADF test helps determine if a time series is stationary.\n",
    "    # If p-value < 0.05, we reject the null hypothesis and conclude that the series is stationary.\n",
    "\n",
    "    # --- Seasonal Decomposition ---\n",
    "    # Decompose the time series into trend, seasonality, and residuals using an additive model\n",
    "    decomposition = seasonal_decompose(df[column], model='additive', period=12)\n",
    "    \n",
    "    # Plot the decomposition results (observed, trend, seasonal, and residual components)\n",
    "    decomposition.plot()\n",
    "    \n",
    "    # Set a title for the decomposition plot\n",
    "    plt.suptitle(f'Seasonal Decomposition of {column}')\n",
    "    \n",
    "    # Display the plots\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the rolling mean with a 12-month window for smoothing time series data\n",
    "rolling_means = df.rolling(window=12).mean()\n",
    "\n",
    "# Create a new figure with a specific size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loop through each column in the DataFrame to plot its rolling mean\n",
    "for column in df.columns:\n",
    "    plt.plot(rolling_means.index, rolling_means[column], label=f'{column} Rolling Mean')\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title('Rolling Mean (12-month window)')\n",
    "\n",
    "# Add a legend to indicate which rolling mean corresponds to which column\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix for all numerical columns in the DataFrame\n",
    "df_corr = df.corr()\n",
    "\n",
    "# Print the correlation matrix to inspect the numerical relationships between variables\n",
    "print(\"Correlation Matrix:\")\n",
    "print(df_corr)\n",
    "\n",
    "# Create a heatmap to visualize the correlation matrix\n",
    "sns.heatmap(df_corr, annot=True, cmap='coolwarm')\n",
    "\n",
    "# Set the title for the heatmap\n",
    "plt.title('Correlation Heatmap')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    plt.figure()\n",
    "    lag_plot(df[column])\n",
    "    plt.title(f'Lag Plot for {column}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(f\"Autocorrelation and Partial Autocorrelation for {column}\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "    plot_acf(df[column], ax=axes[0], lags=20, title=f'ACF: {column}')\n",
    "    plot_pacf(df[column], ax=axes[1], lags=20, title=f'PACF: {column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with lagged versions of the original columns\n",
    "lagged_df = pd.concat(\n",
    "    [df.shift(i).add_suffix(f\"_lag{i}\") for i in range(1, 4)], axis=1  # Lag 1, 2, 3\n",
    ")\n",
    "\n",
    "# Concatenate the original DataFrame with the lagged features and remove any NaN values\n",
    "lagged_df = pd.concat([df, lagged_df], axis=1).dropna()\n",
    "\n",
    "# Display the first few rows of the new DataFrame with lagged values\n",
    "print(lagged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairwise scatter plots for all numerical columns in the DataFrame\n",
    "sns.pairplot(df, kind=\"reg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
