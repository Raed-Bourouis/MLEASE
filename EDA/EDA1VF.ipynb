{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for data analysis and visualization\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt  # For creating various plots and visualizations\n",
    "import numpy as np\n",
    "import pandas as pd  # For handling and manipulating structured data (tables, CSV, etc.)\n",
    "import seaborn as sns  # For advanced data visualization with statistical capabilities\n",
    "# Importing automatic data profiling tool\n",
    "import ydata_profiling as yd  # Used to generate a report with statistics, correlations, and distributions for data exploration\n",
    "# Importing time series analysis tools from pandas and statsmodels\n",
    "from pandas.plotting import lag_plot  # For visualizing lag correlations in time series data\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "# Importing seasonal decomposition tool for time series analysis\n",
    "from statsmodels.tsa.seasonal import  seasonal_decompose  # To decompose a time series into trend, seasonality, and residual components\n",
    "# Importing statistical test for stationarity\n",
    "from statsmodels.tsa.stattools import  adfuller  # Augmented Dickey-Fuller (ADF) test to check stationarity of a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"../datasets/Month_Value_1.csv\",\n",
    "    parse_dates=True,\n",
    ")\n",
    "# Try to detect a datetime column\n",
    "for col in df.columns:\n",
    "    df[col] = pd.to_datetime(\n",
    "        df[col], errors=\"coerce\"\n",
    "    )  # Convert to datetime if possible\n",
    "    if is_datetime64_any_dtype(df[col]):\n",
    "        df.set_index(col, inplace=True)\n",
    "        print(f\"Set '{col}' as the datetime index.\")\n",
    "        break \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProfile = yd.ProfileReport(df)\n",
    "DataProfile.to_file(\"Profile.json\")\n",
    "DataProfile.to_file(\"Profile.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic exploration\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "\n",
    "    for column in df.columns:\n",
    "        df[column] = pd.to_numeric(\n",
    "            df[column],\n",
    "            errors=\"coerce\",\n",
    "        )\n",
    "\n",
    "        if df[column].isna().sum() > 0 or np.isinf(df[column]).sum() > 0:\n",
    "\n",
    "            print(f\"Handling missing values in {column}\")\n",
    "\n",
    "\n",
    "            # Remplacer les valeurs infinies par NaN\n",
    "\n",
    "\n",
    "            df[column] = df[column].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "            # Calcul du ratio de valeurs manquantes\n",
    "\n",
    "            missing_ratio = df[column].isna().sum() / len(df)\n",
    "\n",
    "\n",
    "            # Choix de la stratégie selon le pourcentage de NaN\n",
    "\n",
    "\n",
    "            if missing_ratio < 0.05:\n",
    "\n",
    "                strategy = \"mean\"  # Moins de 5% → Remplacement par la moyenne\n",
    "\n",
    "                df[column].fillna(df[column].mean(), inplace=True)\n",
    "\n",
    "            elif missing_ratio < 0.2:\n",
    "\n",
    "\n",
    "                strategy = \"interpolation\"  # Entre 5% et 20% → Interpolation linéaire\n",
    "\n",
    "                df[column].interpolate(method=\"linear\", inplace=True)\n",
    "\n",
    "            else:\n",
    "\n",
    "\n",
    "                strategy = \"median\"  # Plus de 20% → Remplacement par la médiane\n",
    "\n",
    "                df[column].fillna(df[column].median(), inplace=True)\n",
    "\n",
    "\n",
    "            print(f\"Applied {strategy} strategy for {column}\")\n",
    "\n",
    "    df.to_csv(\"df.csv\")\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# def convert_if_majority_numeric(dataframe):\n",
    "#     # print(dataframe)\n",
    "#     df1 = dataframe\n",
    "#     df1.dropna(inplace=True)\n",
    "#     \"\"\"Convert string numbers to numeric values only if most elements are numeric.\"\"\"\n",
    "#     for column in df1.columns:\n",
    "#         type_counts = df[column].apply(lambda x: is_numeric_dtype(x)).value_counts()\n",
    "#         for x in df[column]:\n",
    "#             print(type(x))\n",
    "\n",
    "#         # Check if majority is numeric\n",
    "#         majority_is_numeric = type_counts.get(True, 0) > type_counts.get(False, 0)\n",
    "#         print(column, \" : \", type_counts.get(True, 0))\n",
    "#         print(\n",
    "#             column,\n",
    "#             \" : \",\n",
    "#         )\n",
    "#         print(column, \" : \", majority_is_numeric)\n",
    "\n",
    "#         if majority_is_numeric:\n",
    "#             df[column] = pd.to_numeric(\n",
    "#                 df[column],\n",
    "#                 errors=\"coerce\",\n",
    "#             )  # Convert only if majority is numeric\n",
    "#             print(\"converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = handle_missing_values(df)\n",
    "def generate_mlops_report(df):\n",
    "    \"\"\"\n",
    "    Generate a machine-interpretable EDA report for MLOps preprocessing\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input time series dataframe\n",
    "    \n",
    "    Returns:\n",
    "    dict: Structured, machine-readable analysis report\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        \"dataset_metadata\": {\n",
    "            \"total_columns\": len(df.columns),\n",
    "            \"total_rows\": len(df),\n",
    "            \"date_range\": {\n",
    "                \"start\": str(df.index.min()),\n",
    "                \"end\": str(df.index.max())\n",
    "            }\n",
    "        },\n",
    "        \"preprocessing_recommendations\": {\n",
    "            \"stationarity\": {},\n",
    "            \"feature_scaling\": [],\n",
    "            \"feature_engineering\": []\n",
    "        },\n",
    "        \"statistical_insights\": {\n",
    "            \"descriptive_stats\": {},\n",
    "            \"correlations\": {\n",
    "                \"significant_correlations\": [],\n",
    "                \"correlation_matrix\": {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Stationarity Analysis\n",
    "    for column in df.columns:\n",
    "        adf_result = adfuller(df[column])\n",
    "        report[\"preprocessing_recommendations\"][\"stationarity\"][column] = {\n",
    "            \"is_stationary\": str(adf_result[1] < 0.05),\n",
    "            \"p_value\": float(adf_result[1]),\n",
    "            \"transformation_needed\": \"YES\" if adf_result[1] >= 0.05 else \"NO\"\n",
    "        }\n",
    "        \n",
    "        # Recommend transformations if not stationary\n",
    "        if adf_result[1] >= 0.05:\n",
    "            report[\"preprocessing_recommendations\"][\"feature_engineering\"].append({\n",
    "                \"column\": column,\n",
    "                \"suggested_transformations\": [\n",
    "                    \"log_transformation\",\n",
    "                    \"differencing\",\n",
    "                    \"rolling_mean_normalization\"\n",
    "                ]\n",
    "            })\n",
    "    \n",
    "    # Descriptive Statistics\n",
    "    for column in df.columns:\n",
    "        report[\"statistical_insights\"][\"descriptive_stats\"][column] = {\n",
    "            \"mean\": float(df[column].mean()),\n",
    "            \"std\": float(df[column].std()),\n",
    "            \"min\": float(df[column].min()),\n",
    "            \"max\": float(df[column].max())\n",
    "        }\n",
    "    \n",
    "    # Correlation Analysis\n",
    "    corr_matrix = df.corr()\n",
    "    report[\"statistical_insights\"][\"correlations\"][\"correlation_matrix\"] = \\\n",
    "        {str(col): {str(subcol): float(corr_matrix.loc[col, subcol]) \n",
    "                    for subcol in df.columns} \n",
    "         for col in df.columns}\n",
    "    \n",
    "    # Significant Correlations\n",
    "    for col1 in df.columns:\n",
    "        for col2 in df.columns:\n",
    "            if col1 != col2:\n",
    "                correlation = float(corr_matrix.loc[col1, col2])\n",
    "                if abs(correlation) > 0.5:\n",
    "                    report[\"statistical_insights\"][\"correlations\"][\"significant_correlations\"].append({\n",
    "                        \"features\": [col1, col2],\n",
    "                        \"correlation\": correlation,\n",
    "                        \"strength\": \"strong\" if abs(correlation) > 0.7 else \"moderate\"\n",
    "                    })\n",
    "    \n",
    "    # Feature Scaling Recommendations\n",
    "    for column in df.columns:\n",
    "        if df[column].std() > 1:  # Suggest scaling for features with high variance\n",
    "            report[\"preprocessing_recommendations\"][\"feature_scaling\"].append({\n",
    "                \"column\": column,\n",
    "                \"recommended_method\": [\"standardization\", \"min_max_scaling\"]\n",
    "            })\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and save the report\n",
    "final_report = generate_mlops_report(df)\n",
    "\n",
    "# Save to JSON\n",
    "with open('mlops_eda_report.json', 'w') as f:\n",
    "    json.dump(final_report, f, indent=4)\n",
    "\n",
    "print(\"MLOps-friendly EDA Report generated and saved to 'mlops_eda_report.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    plt.figure(df.columns.get_loc(column)+1,figsize=(18, 18))\n",
    "    # Plot the time series data for the current column\n",
    "    plt.plot(df.index, df[column], label=column)\n",
    "plt.title('Time Series Plot')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop through each column in the DataFrame to perform stationarity testing and seasonal decomposition\n",
    "for column in df.columns:\n",
    "    print(f\"\\nAnalyzing column: {column}\")  # Print the name of the column being analyzed\n",
    "    \n",
    "    # Create a new figure for the decomposition plot\n",
    "    plt.figure(figsize=(20, 14))\n",
    "\n",
    "    # --- Augmented Dickey-Fuller (ADF) Test for Stationarity ---\n",
    "    adf_result = adfuller(df[column])  # Perform the ADF test on the current column\n",
    "    print(f\"ADF Statistic for {column}: {adf_result[0]}\")  # Print the test statistic\n",
    "    print(f\"p-value for {column}: {adf_result[1]}\")  # Print the p-value (to check stationarity)\n",
    "    \n",
    "    # The ADF test helps determine if a time series is stationary.\n",
    "    # If p-value < 0.05, we reject the null hypothesis and conclude that the series is stationary.\n",
    "\n",
    "    # --- Seasonal Decomposition ---\n",
    "    # Decompose the time series into trend, seasonality, and residuals using an additive model\n",
    "    decomposition = seasonal_decompose(df[column], model='additive', period=12)\n",
    "    \n",
    "    # Plot the decomposition results (observed, trend, seasonal, and residual components)\n",
    "    decomposition.plot()\n",
    "    \n",
    "    # Set a title for the decomposition plot\n",
    "    plt.suptitle(f'Seasonal Decomposition of {column}')\n",
    "    \n",
    "    # Display the plots\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the rolling mean with a 12-month window for smoothing time series data\n",
    "rolling_means = df.rolling(window=12).mean()\n",
    "\n",
    "# Create a new figure with a specific size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loop through each column in the DataFrame to plot its rolling mean\n",
    "for column in df.columns:\n",
    "    plt.plot(rolling_means.index, rolling_means[column], label=f'{column} Rolling Mean')\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title('Rolling Mean (12-month window)')\n",
    "\n",
    "# Add a legend to indicate which rolling mean corresponds to which column\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix for all numerical columns in the DataFrame\n",
    "df_corr = df.corr()\n",
    "\n",
    "# Print the correlation matrix to inspect the numerical relationships between variables\n",
    "print(\"Correlation Matrix:\")\n",
    "print(df_corr)\n",
    "\n",
    "# Create a heatmap to visualize the correlation matrix\n",
    "sns.heatmap(df_corr, annot=True, cmap='coolwarm')\n",
    "\n",
    "# Set the title for the heatmap\n",
    "plt.title('Correlation Heatmap')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    plt.figure()\n",
    "    lag_plot(df[column])\n",
    "    plt.title(f'Lag Plot for {column}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(f\"Autocorrelation and Partial Autocorrelation for {column}\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "    plot_acf(df[column], ax=axes[0], lags=20, title=f'ACF: {column}')\n",
    "    plot_pacf(df[column], ax=axes[1], lags=20, title=f'PACF: {column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with lagged versions of the original columns\n",
    "lagged_df = pd.concat(\n",
    "    [df.shift(i).add_suffix(f\"_lag{i}\") for i in range(1, 4)], axis=1  # Lag 1, 2, 3\n",
    ")\n",
    "\n",
    "# Concatenate the original DataFrame with the lagged features and remove any NaN values\n",
    "lagged_df = pd.concat([df, lagged_df], axis=1).dropna()\n",
    "\n",
    "# Display the first few rows of the new DataFrame with lagged values\n",
    "print(lagged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairwise scatter plots for all numerical columns in the DataFrame\n",
    "sns.pairplot(df, kind=\"reg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
