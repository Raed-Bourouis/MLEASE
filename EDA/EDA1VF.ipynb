{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for data analysis and visualization\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt  # For creating various plots and visualizations\n",
    "import numpy as np\n",
    "import pandas as pd  # For handling and manipulating structured data (tables, CSV, etc.)\n",
    "import seaborn as sns  # For advanced data visualization with statistical capabilities\n",
    "# Importing automatic data profiling tool\n",
    "import ydata_profiling as yd  # Used to generate a report with statistics, correlations, and distributions for data exploration\n",
    "# Importing time series analysis tools from pandas and statsmodels\n",
    "from pandas.plotting import lag_plot  # For visualizing lag correlations in time series data\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "# Importing seasonal decomposition tool for time series analysis\n",
    "from statsmodels.tsa.seasonal import  seasonal_decompose  # To decompose a time series into trend, seasonality, and residual components\n",
    "# Importing statistical test for stationarity\n",
    "from statsmodels.tsa.stattools import  adfuller  # Augmented Dickey-Fuller (ADF) test to check stationarity of a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"../datasets/Month_Value_1.csv\",\n",
    "    parse_dates=True,\n",
    ")\n",
    "# Try to detect a datetime column\n",
    "for col in df.columns:\n",
    "    df[col] = pd.to_datetime(\n",
    "        df[col], errors=\"coerce\"\n",
    "    )  # Convert to datetime if possible\n",
    "    if is_datetime64_any_dtype(df[col]):\n",
    "        df.set_index(col, inplace=True)\n",
    "        print(f\"Set '{col}' as the datetime index.\")\n",
    "        break \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataProfile = yd.ProfileReport(df)\n",
    "# DataProfile.to_file(\"Profile.json\")\n",
    "# DataProfile.to_file(\"Profile.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic exploration\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, validation_fraction=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Applies the best missing value imputation strategy based on MSE.\n",
    "    \n",
    "    This function creates a validation set by artificially masking a fraction of \n",
    "    non-missing values. It then compares several imputation methods by computing \n",
    "    the mean squared error (MSE) between the imputed values and the true values \n",
    "    at the masked positions. The method with the lowest MSE is applied to the \n",
    "    original DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame with possible missing values.\n",
    "    validation_fraction (float): Fraction of non-missing values to mask for validation.\n",
    "    random_state (int): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with missing values imputed.\n",
    "    \"\"\"\n",
    "    # If no missing values, simply return the original dataframe.\n",
    "    if df.isnull().sum().sum() == 0:\n",
    "        print(\"No missing values detected after EDA. Skipping handling.\")\n",
    "        return df\n",
    "    print(\"Missing values detected. Selecting best imputation method.\")\n",
    "\n",
    "    # Create a copy of df to simulate additional missingness for validation.\n",
    "    df_validation = df.copy()\n",
    "    # This mask DataFrame will track the positions that are artificially set to NaN.\n",
    "    mask = pd.DataFrame(False, index=df.index, columns=df.columns)\n",
    "    # For each column, randomly mask a fraction of originally non-missing values.\n",
    "    np.random.seed(random_state)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df_validation[col] = pd.to_numeric(df_validation[col], errors='coerce')\n",
    "        non_missing_indices = df[df[col].notnull()].index\n",
    "        n_to_mask = int(len(non_missing_indices) * validation_fraction)\n",
    "        if n_to_mask > 0:\n",
    "            masked_indices = np.random.choice(non_missing_indices, n_to_mask, replace=False)\n",
    "            mask.loc[masked_indices, col] = True\n",
    "            df_validation.loc[masked_indices, col] = np.nan\n",
    "\n",
    "    # Define candidate imputation methods applied on the artificially masked dataframe.\n",
    "    imputed_dfs = {\n",
    "        \"forward_fill\": df_validation.fillna(method='ffill'),\n",
    "        \"backward_fill\": df_validation.fillna(method='bfill'),\n",
    "        \"mean_imputation\": df_validation.fillna(df_validation.mean()),\n",
    "        \"median_imputation\": df_validation.fillna(df_validation.median()),\n",
    "        \"interpolation\": df_validation.interpolate()\n",
    "    }\n",
    "\n",
    "    mse_scores = {}\n",
    "    # Compute MSE for each method on the positions we masked.\n",
    "    for method_name, imputed_df in imputed_dfs.items():\n",
    "        # y_true holds the ground truth values from the original df where we masked values.\n",
    "        y_true = df[mask]\n",
    "        # y_pred holds the corresponding imputed values.\n",
    "        y_pred = imputed_df[mask]\n",
    "        mse = np.mean((y_true - y_pred) ** 2)\n",
    "        mse_scores[method_name] = mse\n",
    "    print(mse_scores)\n",
    "    best_method = min(mse_scores, key=mse_scores.get)\n",
    "    print(f\"Selected best missing value handling method: {best_method}\")\n",
    "\n",
    "    # Now apply the best method on the original dataframe with actual missing values.\n",
    "    if best_method == \"forward_fill\":\n",
    "        return df.fillna(method='ffill')\n",
    "    elif best_method == \"backward_fill\":\n",
    "        return df.fillna(method='bfill')\n",
    "    elif best_method == \"mean_imputation\":\n",
    "        return df.fillna(df.mean())\n",
    "    elif best_method == \"median_imputation\":\n",
    "        return df.fillna(df.median())\n",
    "    elif best_method == \"interpolation\":\n",
    "        return df.interpolate()\n",
    "    else:\n",
    "        print(\"No valid imputation method selected, returning original df.\")\n",
    "        return df\n",
    "\n",
    "'''\n",
    "def convert_if_majority_numeric(dataframe):\n",
    "    # print(dataframe)\n",
    "    df1 = dataframe\n",
    "    df1.dropna(inplace=True)\n",
    "    \"\"\"Convert string numbers to numeric values only if most elements are numeric.\"\"\"\n",
    "    for column in df1.columns:\n",
    "        type_counts = df[column].apply(lambda x: is_numeric_dtype(x)).value_counts()\n",
    "        for x in df[column]:\n",
    "            print(type(x))\n",
    "\n",
    "        # Check if majority is numeric\n",
    "        majority_is_numeric = type_counts.get(True, 0) > type_counts.get(False, 0)\n",
    "        print(column, \" : \", type_counts.get(True, 0))\n",
    "        print(\n",
    "            column,\n",
    "            \" : \",\n",
    "        )\n",
    "        print(column, \" : \", majority_is_numeric)\n",
    "\n",
    "        if majority_is_numeric:\n",
    "            df[column] = pd.to_numeric(\n",
    "                df[column],\n",
    "                errors=\"coerce\",\n",
    "            )  # Convert only if majority is numeric\n",
    "            print(\"converted\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = handle_missing_values(df)\n",
    "def generate_mlops_report(df):\n",
    "    \"\"\"\n",
    "    Generate a machine-interpretable EDA report for MLOps preprocessing\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input time series dataframe\n",
    "    \n",
    "    Returns:\n",
    "    dict: Structured, machine-readable analysis report\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        \"dataset_metadata\": {\n",
    "            \"total_columns\": len(df.columns),\n",
    "            \"total_rows\": len(df),\n",
    "            \"date_range\": {\n",
    "                \"start\": str(df.index.min()),\n",
    "                \"end\": str(df.index.max())\n",
    "            }\n",
    "        },\n",
    "        \"preprocessing_recommendations\": {\n",
    "            \"stationarity\": {},\n",
    "            \"feature_scaling\": [],\n",
    "            \"feature_engineering\": []\n",
    "        },\n",
    "        \"statistical_insights\": {\n",
    "            \"descriptive_stats\": {},\n",
    "            \"correlations\": {\n",
    "                \"significant_correlations\": [],\n",
    "                \"correlation_matrix\": {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Stationarity Analysis\n",
    "    for column in df.columns:\n",
    "        adf_result = adfuller(df[column])\n",
    "        report[\"preprocessing_recommendations\"][\"stationarity\"][column] = {\n",
    "            \"is_stationary\": str(adf_result[1] < 0.05),\n",
    "            \"p_value\": float(adf_result[1]),\n",
    "            \"transformation_needed\": \"YES\" if adf_result[1] >= 0.05 else \"NO\"\n",
    "        }\n",
    "        \n",
    "        # Recommend transformations if not stationary\n",
    "        if adf_result[1] >= 0.05:\n",
    "            report[\"preprocessing_recommendations\"][\"feature_engineering\"].append({\n",
    "                \"column\": column,\n",
    "                \"suggested_transformations\": [\n",
    "                    \"log_transformation\",\n",
    "                    \"differencing\",\n",
    "                    \"rolling_mean_normalization\"\n",
    "                ]\n",
    "            })\n",
    "    \n",
    "    # Descriptive Statistics\n",
    "    for column in df.columns:\n",
    "        report[\"statistical_insights\"][\"descriptive_stats\"][column] = {\n",
    "            \"mean\": float(df[column].mean()),\n",
    "            \"std\": float(df[column].std()),\n",
    "            \"min\": float(df[column].min()),\n",
    "            \"max\": float(df[column].max())\n",
    "        }\n",
    "    \n",
    "    # Correlation Analysis\n",
    "    corr_matrix = df.corr()\n",
    "    report[\"statistical_insights\"][\"correlations\"][\"correlation_matrix\"] = \\\n",
    "        {str(col): {str(subcol): float(corr_matrix.loc[col, subcol]) \n",
    "                    for subcol in df.columns} \n",
    "         for col in df.columns}\n",
    "    \n",
    "    # Significant Correlations\n",
    "    for col1 in df.columns:\n",
    "        for col2 in df.columns:\n",
    "            if col1 != col2:\n",
    "                correlation = float(corr_matrix.loc[col1, col2])\n",
    "                if abs(correlation) > 0.5:\n",
    "                    report[\"statistical_insights\"][\"correlations\"][\"significant_correlations\"].append({\n",
    "                        \"features\": [col1, col2],\n",
    "                        \"correlation\": correlation,\n",
    "                        \"strength\": \"strong\" if abs(correlation) > 0.7 else \"moderate\"\n",
    "                    })\n",
    "    \n",
    "    # Feature Scaling Recommendations\n",
    "    for column in df.columns:\n",
    "        if df[column].std() > 1:  # Suggest scaling for features with high variance\n",
    "            report[\"preprocessing_recommendations\"][\"feature_scaling\"].append({\n",
    "                \"column\": column,\n",
    "                \"recommended_method\": [\"standardization\", \"min_max_scaling\"]\n",
    "            })\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and save the report\n",
    "final_report = generate_mlops_report(df)\n",
    "\n",
    "# Save to JSON\n",
    "with open('mlops_eda_report.json', 'w') as f:\n",
    "    json.dump(final_report, f, indent=4)\n",
    "\n",
    "print(\"MLOps-friendly EDA Report generated and saved to 'mlops_eda_report.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    plt.figure(df.columns.get_loc(column)+1,figsize=(18, 18))\n",
    "    # Plot the time series data for the current column\n",
    "    plt.plot(df.index, df[column], label=column)\n",
    "plt.title('Time Series Plot')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop through each column in the DataFrame to perform stationarity testing and seasonal decomposition\n",
    "for column in df.columns:\n",
    "    print(f\"\\nAnalyzing column: {column}\")  # Print the name of the column being analyzed\n",
    "    \n",
    "    # Create a new figure for the decomposition plot\n",
    "    plt.figure(figsize=(20, 14))\n",
    "\n",
    "    # --- Augmented Dickey-Fuller (ADF) Test for Stationarity ---\n",
    "    adf_result = adfuller(df[column])  # Perform the ADF test on the current column\n",
    "    print(f\"ADF Statistic for {column}: {adf_result[0]}\")  # Print the test statistic\n",
    "    print(f\"p-value for {column}: {adf_result[1]}\")  # Print the p-value (to check stationarity)\n",
    "    \n",
    "    # The ADF test helps determine if a time series is stationary.\n",
    "    # If p-value < 0.05, we reject the null hypothesis and conclude that the series is stationary.\n",
    "\n",
    "    # --- Seasonal Decomposition ---\n",
    "    # Decompose the time series into trend, seasonality, and residuals using an additive model\n",
    "    decomposition = seasonal_decompose(df[column], model='additive', period=12)\n",
    "    \n",
    "    # Plot the decomposition results (observed, trend, seasonal, and residual components)\n",
    "    decomposition.plot()\n",
    "    \n",
    "    # Set a title for the decomposition plot\n",
    "    plt.suptitle(f'Seasonal Decomposition of {column}')\n",
    "    \n",
    "    # Display the plots\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the rolling mean with a 12-month window for smoothing time series data\n",
    "rolling_means = df.rolling(window=12).mean()\n",
    "\n",
    "# Create a new figure with a specific size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loop through each column in the DataFrame to plot its rolling mean\n",
    "for column in df.columns:\n",
    "    plt.plot(rolling_means.index, rolling_means[column], label=f'{column} Rolling Mean')\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title('Rolling Mean (12-month window)')\n",
    "\n",
    "# Add a legend to indicate which rolling mean corresponds to which column\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix for all numerical columns in the DataFrame\n",
    "df_corr = df.corr()\n",
    "\n",
    "# Print the correlation matrix to inspect the numerical relationships between variables\n",
    "print(\"Correlation Matrix:\")\n",
    "print(df_corr)\n",
    "\n",
    "# Create a heatmap to visualize the correlation matrix\n",
    "sns.heatmap(df_corr, annot=True, cmap='coolwarm')\n",
    "\n",
    "# Set the title for the heatmap\n",
    "plt.title('Correlation Heatmap')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    plt.figure()\n",
    "    lag_plot(df[column])\n",
    "    plt.title(f'Lag Plot for {column}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(f\"Autocorrelation and Partial Autocorrelation for {column}\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "    plot_acf(df[column], ax=axes[0], lags=20, title=f'ACF: {column}')\n",
    "    plot_pacf(df[column], ax=axes[1], lags=20, title=f'PACF: {column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with lagged versions of the original columns\n",
    "lagged_df = pd.concat(\n",
    "    [df.shift(i).add_suffix(f\"_lag{i}\") for i in range(1, 4)], axis=1  # Lag 1, 2, 3\n",
    ")\n",
    "\n",
    "# Concatenate the original DataFrame with the lagged features and remove any NaN values\n",
    "lagged_df = pd.concat([df, lagged_df], axis=1).dropna()\n",
    "\n",
    "# Display the first few rows of the new DataFrame with lagged values\n",
    "print(lagged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairwise scatter plots for all numerical columns in the DataFrame\n",
    "sns.pairplot(df, kind=\"reg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
