{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('../datasets/energydata_complete.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "# Check stationarity for each variable\n",
    "def check_stationarity(series, significance=0.05):\n",
    "    result = adfuller(series)\n",
    "    print(f'ADF Statistic: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    print('Stationary' if result[1] < significance else 'Non-stationary')\n",
    "\n",
    "# Apply to each column\n",
    "# for column in data.columns:\n",
    "#     print(f\"\\nStationarity Test for {column}\")\n",
    "#     check_stationarity(data[column])\n",
    "\n",
    "# Difference data if needed\n",
    "data_diff = data.diff().dropna()\n",
    "\n",
    "# Split data into train and test sets (last 30 periods for testing)\n",
    "train_size = len(data_diff) - 30\n",
    "train, test = data_diff.iloc[:train_size], data_diff.iloc[train_size:]\n",
    "original_test = data.iloc[train_size:]  # For comparison in original scale\n",
    "# Create and fit the VAR model\n",
    "model = VAR(train)\n",
    "\n",
    "# Select order (p) using AIC\n",
    "results = model.select_order()\n",
    "lag_order = results.selected_orders['aic']\n",
    "print(f\"Suggested order (p): {lag_order}\")\n",
    "\n",
    "# Fit the model with the selected order\n",
    "fitted_model = model.fit(lag_order)\n",
    "print(fitted_model.summary())\n",
    "\n",
    "# Forecast for the test period\n",
    "last_train_values = train.values[-lag_order:]\n",
    "forecast_diff = fitted_model.forecast(y=last_train_values, steps=len(test))\n",
    "forecast_diff_df = pd.DataFrame(forecast_diff, index=test.index, columns=train.columns)\n",
    "\n",
    "# Convert forecasts back to original scale\n",
    "# If first differencing was used:\n",
    "forecast_original = pd.DataFrame(index=test.index, columns=data.columns)\n",
    "for col in data.columns:\n",
    "    forecast_original[col] = data.iloc[train_size-1][col] + forecast_diff_df[col].cumsum()\n",
    "\n",
    "# Plot actual vs predicted for each variable\n",
    "def plot_actual_vs_predicted(original_test, forecast_original):\n",
    "    variables = original_test.columns\n",
    "    # n_vars = len(variables)\n",
    "    \n",
    "    for i, col in enumerate(variables):\n",
    "        plt.figure(i+1, figsize=(15, 10))\n",
    "        plt.plot(original_test.index, original_test[col], 'b-', label='Actual')\n",
    "        plt.plot(original_test.index, forecast_original[col], 'r--', label='Predicted')\n",
    "        plt.title(f'Actual vs Predicted: {col}')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Calculate RMSE for this variable\n",
    "        rmse = sqrt(mean_squared_error(original_test[col], forecast_original[col]))\n",
    "        plt.annotate(f'RMSE: {rmse:.4f}', xy=(0.05, 0.85), xycoords='axes fraction')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_actual_vs_predicted(test, forecast_diff_df)\n",
    "\n",
    "# Plot forecast error\n",
    "def plot_forecast_error(original_test, forecast_original):\n",
    "    variables = original_test.columns\n",
    "    n_vars = len(variables)\n",
    "    \n",
    "    for i, col in enumerate(variables):\n",
    "        plt.figure(i+1, figsize=(15, 10))\n",
    "        forecast_error = original_test[col] - forecast_original[col]\n",
    "        plt.plot(original_test.index, forecast_error, 'g-')\n",
    "        plt.axhline(y=0, color='r', linestyle='-')\n",
    "        plt.title(f'Forecast Error: {col}')\n",
    "        \n",
    "        # Calculate mean error and standard deviation\n",
    "        mean_error = forecast_error.mean()\n",
    "        std_error = forecast_error.std()\n",
    "        plt.annotate(f'Mean Error: {mean_error:.4f}\\nStd Dev: {std_error:.4f}', \n",
    "                     xy=(0.05, 0.85), xycoords='axes fraction')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_forecast_error(original_test, forecast_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preprocess_data(df, target_columns, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset:\n",
    "    - Select target columns\n",
    "    - Handle missing values\n",
    "    - Normalize features\n",
    "    - Split into train and test sets\n",
    "    \"\"\"\n",
    "    df = df[target_columns].dropna()\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=target_columns, index=df.index)\n",
    "    \n",
    "    split_idx = int(len(df) * train_ratio)\n",
    "    train, test = df_scaled.iloc[:split_idx], df_scaled.iloc[split_idx:]\n",
    "    \n",
    "    return train, test, scaler\n",
    "\n",
    "def train_var_model(train_data, lags=5):\n",
    "    \"\"\" Train a VAR model for multivariate data \"\"\"\n",
    "    model = VAR()\n",
    "    model_fitted = model.fit(train_data, maxlags=lags)\n",
    "    return model_fitted\n",
    "\n",
    "def train_arima_model(train_series, order=(5,1,0)):\n",
    "    \"\"\" Train an ARIMA model for univariate data \"\"\"\n",
    "    model = ARIMA(order=order, endog=train_series)\n",
    "    model_fitted = model.fit()\n",
    "    return model_fitted\n",
    "\n",
    "def make_predictions(model, train_data, test_data, is_univariate=False):\n",
    "    \"\"\" Generate multi-step forecasts for VAR or ARIMA \"\"\"\n",
    "    if is_univariate:\n",
    "        forecast = model.forecast(steps=len(test_data))\n",
    "        return pd.Series(forecast, index=test_data.index, name=train_data.name)\n",
    "    else:\n",
    "        lag_order = model.k_ar\n",
    "        forecast_input = train_data.values[-lag_order:]\n",
    "        forecast = model.forecast(forecast_input, steps=len(test_data))\n",
    "        return pd.DataFrame(forecast, index=test_data.index, columns=test_data.columns)\n",
    "\n",
    "def evaluate_model(actual, predicted):\n",
    "    \"\"\" Evaluate the model using RMSE for each variable \"\"\"\n",
    "    if isinstance(predicted, pd.Series):\n",
    "        return np.sqrt(mean_squared_error(actual, predicted))\n",
    "    else:\n",
    "        return {col: np.sqrt(mean_squared_error(actual[col], predicted[col])) for col in actual.columns}\n",
    "\n",
    "# Example Usage\n",
    "data = pd.read_csv(\"../datasets/Miles_Traveled.csv\", parse_dates=True, index_col=0)\n",
    "# print(data.columns)\n",
    "\n",
    "target_columns = [data.columns.tolist()[-1]]  # Replace with actual columns\n",
    "\n",
    "train, test, scaler = preprocess_data(data, target_columns)\n",
    "\n",
    "if len(target_columns) == 1:\n",
    "    arima_model = train_arima_model(train[target_columns[0]])\n",
    "    predictions = make_predictions(arima_model, train[target_columns[0]], test[target_columns[0]], is_univariate=True)\n",
    "    predictions_original = scaler.inverse_transform(predictions.values.reshape(-1, 1)).flatten()\n",
    "    predictions_original = pd.Series(predictions_original, index=test.index, name=target_columns[0])\n",
    "else:\n",
    "    var_model = train_var_model(train, lags=5)\n",
    "    predictions = make_predictions(var_model, train, test)\n",
    "    predictions_original = pd.DataFrame(scaler.inverse_transform(predictions), columns=target_columns, index=test.index)\n",
    "\n",
    "# Evaluate\n",
    "errors = evaluate_model(data[target_columns].loc[test.index], predictions_original)\n",
    "print(\"RMSE:\", errors)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12,6))\n",
    "for column in target_columns:\n",
    "    plt.plot(data[column], label=f\"Actual {column}\")\n",
    "    plt.plot(predictions_original[column], linestyle=\"dashed\", label=f\"Predicted {column}\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast vs Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raedb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\raedb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\raedb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\raedb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "C:\\Users\\raedb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 98\u001b[0m\n\u001b[0;32m     93\u001b[0m     predictions_original \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(scaler\u001b[38;5;241m.\u001b[39minverse_transform(predictions), \n\u001b[0;32m     94\u001b[0m                                         columns\u001b[38;5;241m=\u001b[39m[target_column] \u001b[38;5;241m+\u001b[39m feature_columns, \n\u001b[0;32m     95\u001b[0m                                         index\u001b[38;5;241m=\u001b[39mtest\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Evaluate the model's performance (RMSE)\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m errors \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions_original\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Plot the results\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 58\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(actual, predicted)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Evaluate the model using RMSE for each variable \"\"\"\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(predicted, pd\u001b[38;5;241m.\u001b[39mSeries):\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {col: np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(actual[col], predicted[col])) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m actual\u001b[38;5;241m.\u001b[39mcolumns}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:497\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[0;32m    493\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[0;32m    494\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[0;32m    495\u001b[0m         )\n\u001b[1;32m--> 497\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    501\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:104\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m    102\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m    103\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m--> 104\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    107\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1049\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1046\u001b[0m     )\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1049\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.api import VAR, ARIMA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preprocess_data(df, target_column, feature_columns=None, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset:\n",
    "    - Select target and feature columns\n",
    "    - Handle missing values\n",
    "    - Normalize features\n",
    "    - Split into train and test sets\n",
    "    \"\"\"\n",
    "    # If there are features, include them in the dataset, otherwise just use the target column\n",
    "    if feature_columns:\n",
    "        df = df[[target_column] + feature_columns].dropna()\n",
    "    else:\n",
    "        df = df[[target_column]].dropna()\n",
    "\n",
    "    # Scale the features and target\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=[target_column] + (feature_columns or []), index=df.index)\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    split_idx = int(len(df) * train_ratio)\n",
    "    train, test = df_scaled.iloc[:split_idx], df_scaled.iloc[split_idx:]\n",
    "    \n",
    "    return train, test, scaler\n",
    "\n",
    "def train_var_model(train_data, lags=5):\n",
    "    \"\"\" Train a VAR model for multivariate data \"\"\"\n",
    "    model = VAR(train_data)\n",
    "    model_fitted = model.fit(lags)\n",
    "    return model_fitted\n",
    "\n",
    "def train_arima_model(train_series, order=(5, 1, 0)):\n",
    "    \"\"\" Train an ARIMA model for univariate data \"\"\"\n",
    "    model = ARIMA(train_series, order=order)\n",
    "    model_fitted = model.fit()\n",
    "    return model_fitted\n",
    "\n",
    "def make_predictions(model, train_data, test_data, is_univariate=False):\n",
    "    \"\"\" Generate multi-step forecasts for VAR or ARIMA \"\"\"\n",
    "    if is_univariate:\n",
    "        forecast = model.forecast(steps=len(test_data))\n",
    "        return pd.Series(forecast, index=test_data.index, name=train_data.name)\n",
    "    else:\n",
    "        lag_order = model.k_ar\n",
    "        forecast_input = train_data.values[-lag_order:]\n",
    "        forecast = model.forecast(forecast_input, steps=len(test_data))\n",
    "        return pd.DataFrame(forecast, index=test_data.index, columns=test_data.columns)\n",
    "\n",
    "def evaluate_model(actual, predicted):\n",
    "    \"\"\" Evaluate the model using RMSE for each variable \"\"\"\n",
    "    if isinstance(predicted, pd.Series):\n",
    "        return np.sqrt(mean_squared_error(actual, predicted))\n",
    "    else:\n",
    "        return {col: np.sqrt(mean_squared_error(actual[col], predicted[col])) for col in actual.columns}\n",
    "\n",
    "# Example Usage\n",
    "# Example Usage\n",
    "data = pd.read_csv(\"../datasets/Month_Value_1.csv\", parse_dates=True, index_col=0)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Define the target column and features\n",
    "target_column = data.columns[-1]  # Replace with your target column name\n",
    "feature_columns = []  # Replace with your feature column names\n",
    "\n",
    "# Preprocess the data\n",
    "train, test, scaler = preprocess_data(data, target_column, feature_columns)\n",
    "\n",
    "if len(feature_columns) == 0:  # Univariate case (only y)\n",
    "    # Train the ARIMA model\n",
    "    arima_model = train_arima_model(train[target_column])\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = make_predictions(arima_model, train[target_column], test[target_column], is_univariate=True)\n",
    "    \n",
    "    # Inverse transform predictions to the original scale\n",
    "    predictions_original = scaler.inverse_transform(predictions.values.reshape(-1, 1)).flatten()\n",
    "    predictions_original = pd.Series(predictions_original, index=test.index, name=target_column)\n",
    "else:  # Multivariate case (y and X1, X2, ...)\n",
    "    # Train the VAR model\n",
    "    var_model = train_var_model(train, lags=5)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = make_predictions(var_model, train, test)\n",
    "    \n",
    "    # Inverse transform predictions to the original scale\n",
    "    predictions_original = pd.DataFrame(scaler.inverse_transform(predictions), \n",
    "                                        columns=[target_column] + feature_columns, \n",
    "                                        index=test.index)\n",
    "\n",
    "# Evaluate the model's performance (RMSE)\n",
    "errors = evaluate_model(data[[target_column] + (feature_columns or [])].loc[test.index], predictions_original)\n",
    "print(\"RMSE:\", errors)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(data[target_column], label=f\"Actual {target_column}\")\n",
    "plt.plot(predictions_original[target_column], linestyle=\"dashed\", label=f\"Predicted {target_column}\")\n",
    "plt.legend()\n",
    "plt.title(f\"{target_column} Forecast vs Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
